{
  "recentFiles": [
    {
      "basename": "p481",
      "path": "Literature/Notes/p481.md"
    },
    {
      "basename": "ğŸ§ª--PhD-NTNU--Projects--Sleeve-Slip-Problem",
      "path": "NTNU/Org/ğŸ§ªProjects/Sleeve-Slip--DigiWells/ğŸ§ª--PhD-NTNU--Projects--Sleeve-Slip-Problem.md"
    },
    {
      "basename": "ğŸ”—NTNU",
      "path": "NTNU/Org/ğŸ”—NTNU.md"
    },
    {
      "basename": "interpretations--RL",
      "path": "P-Tasks/â•/ğŸ‘¨â€ğŸ“/interpretations--RL.md"
    },
    {
      "basename": "eq--Semi-gradient TD(0) update",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/update equations/eq--Semi-gradient TD(0) update.md"
    },
    {
      "basename": "datav--Pubs--DIGIWELLS--ABS",
      "path": "Literature/datav--Pubs--DIGIWELLS--ABS.md"
    },
    {
      "basename": "DQN--code",
      "path": "Literature/Notes/ğŸ”—/ğŸ‘©â€ğŸ’»/GitHub/DQN--code.md"
    },
    {
      "basename": "Untitled",
      "path": "NTNU/Org/ğŸ§ªProjects/Sleeve-Slip--DigiWells/Untitled.md"
    },
    {
      "basename": "Software",
      "path": "Literature/Notes/Sources/Software.md"
    },
    {
      "basename": "ELIGIBILITY TRACES",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/ELIGIBILITY TRACES.md"
    },
    {
      "basename": "CMD__GET_PUB",
      "path": "CMD__GET_PUB.md"
    },
    {
      "basename": "Stochastic-gradient-descent SGD",
      "path": "Literature/Theory/Theory/Stochastic-gradient-descent SGD.md"
    },
    {
      "basename": "p220--Proof-Convergence-Linear-TD(0)",
      "path": "Literature/Notes/p220--Proof-Convergence-Linear-TD(0).md"
    },
    {
      "basename": "p220",
      "path": "Literature/Notes/p220.md"
    },
    {
      "basename": "p375",
      "path": "Literature/Notes/p375.md"
    },
    {
      "basename": "p220--Monte Carlo is true SGD",
      "path": "Literature/Notes/p220--Monte Carlo is true SGD.md"
    },
    {
      "basename": "p220--79",
      "path": "Literature/Notes/p220--79.md"
    },
    {
      "basename": "p220--77",
      "path": "Literature/Notes/p220--77.md"
    },
    {
      "basename": "ğŸ”—ğŸ‘¨â€ğŸ“ Reinforcement Learning Code Examples",
      "path": "Literature/Notes/ğŸ”—/ğŸ‘©â€ğŸ’»/GitHub/ğŸ”—ğŸ‘¨â€ğŸ“ Reinforcement Learning Code Examples.md"
    },
    {
      "basename": "âš ğŸ¤·â€â™‚ï¸1",
      "path": "Literature/Notes/ğŸ’­ Arguments/mismatches with my understanding-opinion/âš ğŸ¤·â€â™‚ï¸1.md"
    },
    {
      "basename": "Sliding-Mode-based--Extremum-Seeking",
      "path": "Literature/Theory/Methods/Sliding-Mode-based--Extremum-Seeking.md"
    },
    {
      "basename": "p478",
      "path": "Literature/Notes/p478.md"
    },
    {
      "basename": "ğŸ¤”--HOW DOES DQN PULL THOSE OFFâ“â“",
      "path": "Literature/Notes/ğŸ’­ Arguments/mismatches with my understanding-opinion/ğŸ¤”--HOW DOES DQN PULL THOSE OFFâ“â“.md"
    },
    {
      "basename": "ğŸ¨Ohio-State-uni",
      "path": "NTNU/Org/ğŸ§‘People/ğŸ¨Ohio-State-uni.md"
    },
    {
      "basename": "ğŸ¨Deep-Mind",
      "path": "NTNU/Org/ğŸ§‘People/ğŸ¨Deep-Mind.md"
    },
    {
      "basename": "datav--Tasks",
      "path": "datav--Tasks.md"
    },
    {
      "basename": "p478--3",
      "path": "Literature/Notes/p478--3.md"
    },
    {
      "basename": "eq--RL--weight-update-equation--backward-view-eligibility-traces",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/update equations/eq--RL--weight-update-equation--backward-view-eligibility-traces.md"
    },
    {
      "basename": "eq--RL--eligibility-trace-update-equation",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/update equations/eq--RL--eligibility-trace-update-equation.md"
    },
    {
      "basename": "âš RL--training-dataset-is-not-static",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/âš RL--training-dataset-is-not-static.md"
    },
    {
      "basename": "âš RL--sensitivity-to-hyperparameters",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/âš RL--sensitivity-to-hyperparameters.md"
    },
    {
      "basename": "eq--RL--proof-ratio",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/eq--RL--proof-ratio.md"
    },
    {
      "basename": "eq--RL--learning-rate",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/eq--RL--learning-rate.md"
    },
    {
      "basename": "eq--RL--greedy-action-selection",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/eq--RL--greedy-action-selection.md"
    },
    {
      "basename": "eq--RL--gamma-discounted-future-state-distribution",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/eq--RL--gamma-discounted-future-state-distribution.md"
    },
    {
      "basename": "eq--RL--Policy-Advantage",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/eq--RL--Policy-Advantage.md"
    },
    {
      "basename": "Connection between value function and action-value function",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/Connection between value function and action-value function.md"
    },
    {
      "basename": "Bellman-Equation",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/Bellman-Equation.md"
    },
    {
      "basename": "eq--Mean-Square-TD-Error",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/eq--Mean-Square-TD-Error.md"
    },
    {
      "basename": "actual-optimal-policy",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/actual-optimal-policy.md"
    },
    {
      "basename": "eq--RL--Stochastic-Gradient-Descent",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/update equations/eq--RL--Stochastic-Gradient-Descent.md"
    },
    {
      "basename": "eq--RL--policy-gradient-general-update",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/update equations/eq--RL--policy-gradient-general-update.md"
    },
    {
      "basename": "eq--expected update for state-action pair",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/update equations/eq--expected update for state-action pair.md"
    },
    {
      "basename": "eq--TD-error",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/error terms/eq--TD-error.md"
    },
    {
      "basename": "eq--RL--Return-Error",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/error terms/eq--RL--Return-Error.md"
    },
    {
      "basename": "eq--RL--n_step-error",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/error terms/eq--RL--n_step-error.md"
    },
    {
      "basename": "eq--RL--weight-update--truncated-Î»-return",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/update equations/eq--RL--weight-update--truncated-Î»-return.md"
    },
    {
      "basename": "eq--RL--REINFORCE-update",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/update equations/eq--RL--REINFORCE-update.md"
    },
    {
      "basename": "eq--RL--truncated-Î»-return",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/update equations/eq--RL--truncated-Î»-return.md"
    },
    {
      "basename": "eq--RL--sampled update for state-action pair",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/update equations/eq--RL--sampled update for state-action pair.md"
    }
  ],
  "omittedPaths": [],
  "maxLength": null,
  "openType": "tab"
}