{
  "recentFiles": [
    {
      "basename": "datav--Tasks",
      "path": "datav--Tasks.md"
    },
    {
      "basename": "LAB",
      "path": "LAB.md"
    },
    {
      "basename": "ğŸ”—NTNU",
      "path": "NTNU/Org/ğŸ”—NTNU.md"
    },
    {
      "basename": "Write my RL theory questions and send to Ole and Bjarne",
      "path": "P-Tasks/â•/Write my RL theory questions and send to Ole and Bjarne.md"
    },
    {
      "basename": "datav--Tasks--organizing--has-deadline",
      "path": "datav--Tasks--organizing--has-deadline.md"
    },
    {
      "basename": "reimburcement of December 2022 Stavanger trip",
      "path": "P-Tasks/â•/reimburcement of December 2022 Stavanger trip.md"
    },
    {
      "basename": "ğŸ§‘ğŸ¨NTNU Hjelp",
      "path": "NTNU/Org/ğŸ§‘People/ğŸ§‘ğŸ¨NTNU Hjelp.md"
    },
    {
      "basename": "ğŸ”²5",
      "path": "ğŸ”²5.md"
    },
    {
      "basename": "Reinforcement Learning Map",
      "path": "Literature/Theory/Reinforcement Learning/Reinforcement Learning Map.canvas"
    },
    {
      "basename": "value-functions-approximation",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/value-functions-approximation.md"
    },
    {
      "basename": "âš  Vanishing Gradient problem",
      "path": "Literature/Notes/âš  Warnings/Implementations/âš  Vanishing Gradient problem.md"
    },
    {
      "basename": "ğŸ’­ Against--RL--Monte-Carlo",
      "path": "Literature/Notes/ğŸ’­ Arguments/against/ğŸ’­ Against--RL--Monte-Carlo.md"
    },
    {
      "basename": "eq--P_hat--play",
      "path": "Literature/Notes/Equations/eq--P_hat--play.md"
    },
    {
      "basename": "eq--Sylvester-Equation",
      "path": "Literature/Notes/Equations/eq--Sylvester-Equation.md"
    },
    {
      "basename": "eq--proof-ratio-1",
      "path": "Literature/Notes/Equations/eq--proof-ratio-1.pdf"
    },
    {
      "basename": "â˜¹ disadvantages",
      "path": "P-Tasks/â•/ğŸ‘¨ğŸ¼â€ğŸ“/ğŸ§ª/â˜¹ disadvantages.md"
    },
    {
      "basename": "datav--Pubs-all",
      "path": "Literature/datav--Pubs-all.md"
    },
    {
      "basename": "âš  Potential change of offices",
      "path": "P-Tasks/â•/âš  Potential change of offices.md"
    },
    {
      "basename": "ğŸ§‘Berit-Dahl",
      "path": "NTNU/Org/ğŸ§‘People/ğŸ§‘Berit-Dahl.md"
    },
    {
      "basename": "âš  Seek advice on dealing with lack of respect",
      "path": "P-Tasks/â•/âš  Seek advice on dealing with lack of respect.md"
    },
    {
      "basename": "p372--monte carlo has high variance",
      "path": "Literature/Notes/p372--monte carlo has high variance.md"
    },
    {
      "basename": "â•â“why-is-that--bias-in-TD-learning",
      "path": "Literature/Notes/â•â“why-is-that--bias-in-TD-learning.md"
    },
    {
      "basename": "ğŸ§‘Ole-Morten",
      "path": "NTNU/Org/ğŸ§‘People/ğŸ§‘Ole-Morten.md"
    },
    {
      "basename": "âš ï¸âš ï¸âš ï¸ Wait for refund from EECI for transaction mistake",
      "path": "P-Tasks/â•/âš ï¸âš ï¸âš ï¸ Wait for refund from EECI for transaction mistake.md"
    },
    {
      "basename": "ğŸ§‘ğŸ¨EECI",
      "path": "NTNU/Org/ğŸ§‘People/ğŸ§‘ğŸ¨EECI.md"
    },
    {
      "basename": "datav--Pubs",
      "path": "Literature/datav--Pubs.md"
    },
    {
      "basename": "check if eligible for Sit-exam for Adaptive Control",
      "path": "P-Tasks/âœ”/check if eligible for Sit-exam for Adaptive Control.md"
    },
    {
      "basename": "ORG -- DISTRIBUTED CONTROL COURSE",
      "path": "P-Tasks/âœ”/ORG -- DISTRIBUTED CONTROL COURSE.md"
    },
    {
      "basename": "ğŸ“ğŸ”—",
      "path": "Automations/ğŸ“ğŸ”—.md"
    },
    {
      "basename": "â•ğŸ“§--Quick",
      "path": "Automations/â•ğŸ“§--Quick.md"
    },
    {
      "basename": "p220--On policy VS off policy",
      "path": "Literature/Notes/p220--On policy VS off policy.md"
    },
    {
      "basename": "has--logic",
      "path": "Literature/Properties/in math/has--logic.md"
    },
    {
      "basename": "booking flights and Hotel to Paris",
      "path": "P-Tasks/â•/booking flights and Hotel to Paris.md"
    },
    {
      "basename": "Computational benefit of Temporal Difference Learning",
      "path": "Literature/Notes/Computational benefit of Temporal Difference Learning.md"
    },
    {
      "basename": "EDUCATIONAL",
      "path": "P-Tasks/â•/ALL/EDUCATIONAL.md"
    },
    {
      "basename": "check-questions-noted-while-reading",
      "path": "P-Tasks/â•/ğŸ‘¨ğŸ¼â€ğŸ“/check-questions-noted-while-reading.md"
    },
    {
      "basename": "ğŸ’­ Against--RL--expected-updates",
      "path": "Literature/Notes/ğŸ’­ Arguments/against/ğŸ’­ Against--RL--expected-updates.md"
    },
    {
      "basename": "eq--expected update for state-action pair",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/update equations/eq--expected update for state-action pair.md"
    },
    {
      "basename": "sampling error",
      "path": "Literature/Notes/sampling error.md"
    },
    {
      "basename": "p379--1",
      "path": "Literature/Notes/p379--1.md"
    },
    {
      "basename": "p220--37",
      "path": "Literature/Notes/p220--37.md"
    },
    {
      "basename": "environment-dynamics",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/environment-dynamics.md"
    },
    {
      "basename": "MONTE CARLO METHODS",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/MONTE CARLO METHODS.md"
    },
    {
      "basename": "p220",
      "path": "Literature/Notes/p220.md"
    },
    {
      "basename": "p220--non learnable quantities in Reinforcement Learning Exist",
      "path": "Literature/Notes/p220--non learnable quantities in Reinforcement Learning Exist.md"
    },
    {
      "basename": "p220--how to achieve unbiased expectation sampling",
      "path": "Literature/Notes/p220--how to achieve unbiased expectation sampling.md"
    },
    {
      "basename": "p220--89",
      "path": "Literature/Notes/p220--89.md"
    },
    {
      "basename": "Stochastic-gradient-descent SGD",
      "path": "Literature/Theory/Theory/Stochastic-gradient-descent SGD.md"
    },
    {
      "basename": "eq--RL--Residual-Gradient-Algorithm-update",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/eq--RL--Residual-Gradient-Algorithm-update.md"
    },
    {
      "basename": "eq--Mean-Square-TD-Error",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/eq--Mean-Square-TD-Error.md"
    }
  ],
  "omittedPaths": [],
  "maxLength": null,
  "openType": "tab"
}