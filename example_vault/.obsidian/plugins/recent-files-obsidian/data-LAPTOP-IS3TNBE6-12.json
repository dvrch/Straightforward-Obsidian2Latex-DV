{
  "recentFiles": [
    {
      "basename": "p220",
      "path": "Literature/Notes/p220.md"
    },
    {
      "basename": "âš ğŸ¤·â€â™‚ï¸1",
      "path": "Literature/Notes/ğŸ’­ Arguments/mismatches with my understanding-opinion/âš ğŸ¤·â€â™‚ï¸1.md"
    },
    {
      "basename": "datav--Tasks",
      "path": "datav--Tasks.md"
    },
    {
      "basename": "PAY FOR DISTRIBUTED CONTROL COURSE",
      "path": "P-Tasks/âœ”/PAY FOR DISTRIBUTED CONTROL COURSE.md"
    },
    {
      "basename": "p379--1",
      "path": "Literature/Notes/p379--1.md"
    },
    {
      "basename": "datav--Pubs",
      "path": "Literature/datav--Pubs.md"
    },
    {
      "basename": "write about experience",
      "path": "P-Tasks/âœ”/write about experience.md"
    },
    {
      "basename": "ğŸ‘¨â€ğŸ“--adaptive-control",
      "path": "P-Tasks/â•/ğŸ‘¨ğŸ¼â€ğŸ“/ğŸ‘¨â€ğŸ“--adaptive-control.md"
    },
    {
      "basename": "â¡ download and read citation",
      "path": "P-Tasks/â•/â¡ download and read citation.md"
    },
    {
      "basename": "â•ğŸ” check more trustworthy source",
      "path": "P-Tasks/â•/Untitled/â•ğŸ” check more trustworthy source.md"
    },
    {
      "basename": "â•ğŸ” continue internet search",
      "path": "P-Tasks/â•/Untitled/â•ğŸ” continue internet search.md"
    },
    {
      "basename": "LAB",
      "path": "LAB.md"
    },
    {
      "basename": "p220--how to achieve unbiased expectation sampling",
      "path": "Literature/Notes/p220--how to achieve unbiased expectation sampling.md"
    },
    {
      "basename": "has--logic",
      "path": "Literature/Properties/in math/has--logic.md"
    },
    {
      "basename": "p220--89",
      "path": "Literature/Notes/p220--89.md"
    },
    {
      "basename": "Stochastic-gradient-descent SGD",
      "path": "Literature/Theory/Theory/Stochastic-gradient-descent SGD.md"
    },
    {
      "basename": "eq--RL--Residual-Gradient-Algorithm-update",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/eq--RL--Residual-Gradient-Algorithm-update.md"
    },
    {
      "basename": "eq--Mean-Square-TD-Error",
      "path": "Literature/Theory/Theory/Reinforcement-Learning/Equations/eq--Mean-Square-TD-Error.md"
    },
    {
      "basename": "p379",
      "path": "Literature/Notes/p379.md"
    },
    {
      "basename": "Reinforcement learning--Self-study-report",
      "path": "P-Tasks/â•/ğŸ‘¨â€ğŸ“/Reinforcement learning--Self-study-report.md"
    },
    {
      "basename": "p220--stochastic gradient descent",
      "path": "Literature/Notes/p220--stochastic gradient descent.md"
    },
    {
      "basename": "p220--90",
      "path": "Literature/Notes/p220--90.md"
    },
    {
      "basename": "p220--On policy VS off policy",
      "path": "Literature/Notes/p220--On policy VS off policy.md"
    },
    {
      "basename": "ğŸ§ª--PhD-NTNU--Self-Study--Reinforcement-Learning--Pub-Reading",
      "path": "NTNU/Org/ğŸ§ªProjects/ğŸ§ª--PhD-NTNU--Self-Study--Reinforcement-Learning--Pub-Reading.md"
    },
    {
      "basename": "Why--are those convergence properties like this",
      "path": "P-Tasks/â•/ğŸ‘¨â€ğŸ“/Why--are those convergence properties like this.md"
    },
    {
      "basename": "ğŸ’­ Against--RL--Îµ-greedy",
      "path": "Literature/Notes/ğŸ’­ Arguments/against/ğŸ’­ Against--RL--Îµ-greedy.md"
    },
    {
      "basename": "has--intuition",
      "path": "Literature/Properties/in Pubs/has--intuition.md"
    },
    {
      "basename": "p220--75",
      "path": "Literature/Notes/p220--75.md"
    },
    {
      "basename": "ğŸ§‘Kristian-Lovland",
      "path": "NTNU/Org/ğŸ§‘People/ğŸ§‘Kristian-Lovland.md"
    },
    {
      "basename": "Computational benefit of Temporal Difference Learning",
      "path": "Literature/Notes/Computational benefit of Temporal Difference Learning.md"
    },
    {
      "basename": "p220--backprop no work for deeper nns",
      "path": "Literature/Notes/p220--backprop no work for deeper nns.md"
    },
    {
      "basename": "p220--28",
      "path": "Literature/Notes/p220--28.md"
    },
    {
      "basename": "p220--âš Modeling error danger",
      "path": "Literature/Notes/p220--âš Modeling error danger.md"
    },
    {
      "basename": "p371",
      "path": "Literature/Notes/p371.md"
    },
    {
      "basename": "p372",
      "path": "Literature/Notes/p372.md"
    },
    {
      "basename": "p373",
      "path": "Literature/Notes/p373.md"
    },
    {
      "basename": "p374",
      "path": "Literature/Notes/p374.md"
    },
    {
      "basename": "p375",
      "path": "Literature/Notes/p375.md"
    },
    {
      "basename": "p376",
      "path": "Literature/Notes/p376.md"
    },
    {
      "basename": "p377",
      "path": "Literature/Notes/p377.md"
    },
    {
      "basename": "p378",
      "path": "Literature/Notes/p378.md"
    },
    {
      "basename": "p379--curse of dimensionality",
      "path": "Literature/Notes/p379--curse of dimensionality.md"
    },
    {
      "basename": "p376--2",
      "path": "Literature/Notes/p376--2.md"
    },
    {
      "basename": "CMD__GET_PUB",
      "path": "CMD__GET_PUB.md"
    },
    {
      "basename": "p375--3",
      "path": "Literature/Notes/p375--3.md"
    },
    {
      "basename": "p375--Semi-gradient-methods-lack-theoretical-support---2020",
      "path": "Literature/Notes/p375--Semi-gradient-methods-lack-theoretical-support---2020.md"
    },
    {
      "basename": "p371--suboptimal policies",
      "path": "Literature/Notes/p371--suboptimal policies.md"
    },
    {
      "basename": "p372--monte carlo has high variance",
      "path": "Literature/Notes/p372--monte carlo has high variance.md"
    },
    {
      "basename": "â•â“why-is-that--bias-in-TD-learning",
      "path": "Literature/Notes/â•â“why-is-that--bias-in-TD-learning.md"
    },
    {
      "basename": "ğŸ§‘Ole-Morten",
      "path": "NTNU/Org/ğŸ§‘People/ğŸ§‘Ole-Morten.md"
    }
  ],
  "omittedPaths": [],
  "maxLength": null,
  "openType": "tab"
}